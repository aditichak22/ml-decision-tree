{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOd/T8txvsHIrSzlniZ5GnU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aditichak22/ml-decision-tree/blob/main/Decision_Tree_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCitn9Z4lV4Z",
        "outputId": "05b07f45-a3da-4265-de43-b7c30bcb85b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def prepareData(datafile):\n",
        "  data = pd.read_csv(datafile, index_col=False, names=[\"Y\", \"x1\", \"x2\", \"x3\", \"x4\", \"x5\", \"x6\", \"x7\", \"x8\", \"x9\", \"x10\", \"x11\", \"x12\", \"x13\", \"x14\", \"x15\", \"x16\", \"x17\", \"x18\", \"x19\", \"x20\", \"x21\", \"x22\"])\n",
        "\n",
        "  data['Y'][data['Y'] == 0] = -1\n",
        "  X = data.iloc[:, 1:23]\n",
        "  y = data.iloc[:, 0:1]\n",
        "\n",
        "  # m = no. of training samples, n = no. of features\n",
        "  m,n = X.shape\n",
        "  return data, X, y\n",
        "\n",
        "data, X, y = prepareData(\"/content/drive/MyDrive/heart_train.data\")\n",
        "test_data, test_X, test_y = prepareData(\"/content/drive/MyDrive/heart_test.data\")"
      ],
      "metadata": {
        "id": "tn9nGVARlZGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "DonJQllyoRQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-o_3tcXWyIO5",
        "outputId": "81c3e087-b6f8-4f0a-af24-4e520d1a6a05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'x1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Node(object):    \n",
        "    \n",
        "    def __init__(self, data, parent_attr_value):\n",
        "        self.data = data\n",
        "        self.children = []\n",
        "        self.attr = None\n",
        "        self.decision = None\n",
        "        self.parent_attr = None\n",
        "        self.parent_attr_value = parent_attr_value\n",
        "\n",
        "class DT(object):\n",
        "    \n",
        "    def __init__(self, data, attr, is_0p, is_all_pos):\n",
        "        self.data = data\n",
        "        self.attr = attr\n",
        "        self.is_0p = is_0p\n",
        "        self.is_all_pos = is_all_pos\n",
        "        self.root = None\n",
        "        self.training_error = None\n",
        "        \n",
        "    def classify(self):\n",
        "        node = Node(self.data, None)\n",
        "        self.root = node \n",
        "        self.build_tree(node)\n",
        "        \n",
        "    def build_tree(self, node):        \n",
        "        data = node.data\n",
        "        attr = self.attr\n",
        "        is_0p = self.is_0p\n",
        "        is_all_pos = self.is_all_pos\n",
        "        \n",
        "        if is_all_pos is not None:\n",
        "            node.decision = 1 if is_all_pos else -1\n",
        "            node.attr = 'POSITIVE' if is_all_pos else 'NEGATIVE'\n",
        "            return\n",
        "        \n",
        "        attr_values = data[attr].unique()    \n",
        "        node.attr = attr\n",
        "        \n",
        "        for attr_value in attr_values:\n",
        "            split = data[data[attr] == attr_value]\n",
        "            child = Node(split, attr_value)\n",
        "            child.parent_attr = attr\n",
        "            node.children.append(child)\n",
        "            \n",
        "            if attr_value == 0 and is_0p: \n",
        "                child.decision = 1\n",
        "            elif attr_value == 1 and is_0p:\n",
        "                child.decision = -1\n",
        "            elif attr_value == 0 and not is_0p:\n",
        "                child.decision = -1\n",
        "            elif attr_value == 1 and not is_0p:\n",
        "                child.decision = 1\n",
        "                \n",
        "    \n",
        "    def predict_dataset(self, data):    \n",
        "        Y_pred = []\n",
        "        for i in range(0, len(data)):\n",
        "            prediction = self.predict(self.root, data.iloc[i])\n",
        "            Y_pred.append(prediction)\n",
        "        return Y_pred\n",
        "    \n",
        "    def predict(self, node, data_point):        \n",
        "        if node.decision is not None:\n",
        "            return node.decision    \n",
        "        attr = node.attr            \n",
        "        child = None\n",
        "        for child in node.children:        \n",
        "            if child.parent_attr_value == data_point[attr]:\n",
        "                break              \n",
        "        return self.predict(child, data_point)      "
      ],
      "metadata": {
        "id": "khoWSRMLctIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_hypotheses(data):\n",
        "    classifiers = []\n",
        "    \n",
        "    all_positive = DT(data, None, None, True)\n",
        "    all_negative = DT(data, None, None, False)\n",
        "    \n",
        "    classifiers.append(all_negative)\n",
        "    classifiers.append(all_positive)\n",
        "    \n",
        "    for attr in data.columns:\n",
        "        if attr == 'Y' or attr == 'W':\n",
        "            continue\n",
        "        \n",
        "        classifier = DT(data, attr, True, None)\n",
        "        classifiers.append(classifier)\n",
        "        \n",
        "        classifier = DT(data, attr, False, None)\n",
        "        classifiers.append(classifier)\n",
        "        \n",
        "    for i in range(len(classifiers)):\n",
        "        classifiers[i].classify()\n",
        "        \n",
        "    return classifiers\n",
        "\n",
        "def fetchMinAlpha(alphas, classifiers, data, position):\n",
        "    current_alpha = alphas[position]    \n",
        "    M,N = data.shape\n",
        "    \n",
        "    Y = data['Y'].values.reshape(M, 1)\n",
        "    Y_pred = np.array(classifiers[position].predict_dataset(data)).reshape(M, 1)\n",
        "    misclassifications = abs(Y-Y_pred)\n",
        "    \n",
        "    misclassifications_indices = np.where(misclassifications > 0)[0]\n",
        "    correct_classification_indices = np.where(misclassifications <= 0)[0]\n",
        "    \n",
        "    misclassified_data = data.iloc[misclassifications_indices]\n",
        "    correctly_classified_data = data.iloc[correct_classification_indices]\n",
        "    \n",
        "    misclassified_data_length = len(misclassifications_indices)\n",
        "    correctly_classified_data_length = len(correct_classification_indices)\n",
        "    \n",
        "    Y_misclassified = data['Y'].iloc[misclassifications_indices].values.reshape((misclassified_data_length,1))\n",
        "    Y_correctly_classified = data['Y'].iloc[correct_classification_indices].values.reshape((correctly_classified_data_length,1))  \n",
        "    \n",
        "    Y_pred_misclassified = np.zeros((misclassified_data_length,1))\n",
        "    Y_pred_correctly_classified = np.zeros((correctly_classified_data_length, 1))\n",
        "    \n",
        "    for i in range(len(alphas)):        \n",
        "        if i == position:\n",
        "            continue\n",
        "        \n",
        "        alpha = alphas[i]\n",
        "        Y_pred_misclassified = Y_pred_misclassified + alpha*(np.array(classifiers[i].predict_dataset(misclassified_data)).reshape(misclassified_data_length,1))\n",
        "        Y_pred_correctly_classified = Y_pred_correctly_classified + alpha*(np.array(classifiers[i].predict_dataset(correctly_classified_data)).reshape(correctly_classified_data_length,1))\n",
        "        \n",
        "    correctly_classified = np.exp(-1.0 * Y_correctly_classified * Y_pred_correctly_classified).sum()\n",
        "    misclassified = np.exp(-1.0 * Y_misclassified * Y_pred_misclassified).sum()\n",
        "    \n",
        "    modified_alpha = 0.5 * np.log(correctly_classified/misclassified)\n",
        "    \n",
        "    return (alphas, modified_alpha, modified_alpha-current_alpha)\n",
        "    \n",
        "\n",
        "def fetchNextMinAdjustedAlphas(alphas, classifiers, data, counter):\n",
        "    N_c = len(alphas)    \n",
        "    current = counter + 1\n",
        "    isRoundRobinComplete = False    \n",
        "    while not isRoundRobinComplete :\n",
        "        (alphas, modified_alpha, difference) = fetchMinAlpha(alphas, classifiers, data, (current%N_c))\n",
        "        print('C ALPHA ', alphas[current%N_c], 'M ALPHA', modified_alpha, 'DIFFERENCE ', difference, ', POSITION ', current)\n",
        "        if abs(difference) > 1e-4:\n",
        "            alphas[current%N_c] = modified_alpha\n",
        "            return (alphas, current%N_c)\n",
        "        \n",
        "        if current-counter > N_c:\n",
        "            isRoundRobinComplete = True\n",
        "            return (alphas, None)\n",
        "        \n",
        "        current = current+1   \n",
        "\n",
        "def coordinate_descent(data, classifiers):    \n",
        "    N_c = len(classifiers)\n",
        "    alphas = np.zeros(N_c).reshape(N_c,1)\n",
        "    \n",
        "    iterations = counter = 0\n",
        "    is_local_optimum = False\n",
        "    while not is_local_optimum:\n",
        "        (alphas, counter) = fetchNextMinAdjustedAlphas(alphas, classifiers, data, counter)\n",
        "        \n",
        "        iterations = iterations+1\n",
        "        if counter is None:\n",
        "            is_local_optimum = True\n",
        "            break\n",
        "        \n",
        "        print('--------------------------------------------------------')\n",
        "        print('ITERATION ', iterations, ', ATTRIBUTE ', counter)\n",
        "        print('--------------------------------------------------------')\n",
        "    \n",
        "    print('ALPHAS ', alphas)\n",
        "    \n",
        "    return alphas\n",
        "\n",
        "def accuracy(data, alphas, classifiers):\n",
        "    M, N = data.shape\n",
        "    Y = data['Y'].values.reshape(M,1)\n",
        "    Y_pred = np.zeros(M).reshape(M,1)\n",
        "    \n",
        "    N_c = len(classifiers)\n",
        "    \n",
        "    for i in range(N_c):\n",
        "        alpha = alphas[i]\n",
        "        Y_pred = Y_pred + alpha*(np.array(classifiers[i].predict_dataset(data)).reshape(M,1))\n",
        "        \n",
        "    \n",
        "    Y_pred[Y_pred < 0] = -1\n",
        "    Y_pred[Y_pred >= 0] = 1\n",
        "    #print(Y-Y_pred)\n",
        "    \n",
        "    misclassification = sum(abs(Y-Y_pred))/2\n",
        "    \n",
        "    return (1.0 - misclassification/M ) * 100\n",
        "\n",
        "classifiers = build_hypotheses(data)\n",
        "alphas = coordinate_descent(data, classifiers)\n",
        "training_accuracy = accuracy(data, alphas, classifiers)\n",
        "print('Training accuracy : ', training_accuracy)\n",
        "test_accuracy = accuracy(test_data, alphas, classifiers)\n",
        "print('Test accuracy : ', test_accuracy)\n",
        "\n",
        "        \n",
        "        "
      ],
      "metadata": {
        "id": "9QuSmyjXc0LT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 3\n",
        "\n",
        "\n",
        "2a) \n",
        "\n",
        "Optimal values of alpha [-7.572, -0.013, 2.21, 0, 0.487, 0, 5.279, 0, -0.811, 0,\n",
        "-0.316, 0, -0.131, 0, -0.6, 0, -4.88, 0, 3.589, 0, -2.079, 0, -0.392, 0, 0.447, 0, -0.794, 0, -3.06, 0, -1.19, 0, -0.744, 0, -2.952, 0, -1.898, 0, -0.05, 0, -0.312, 0 -0.159, 0, -0.377]\n",
        "\n",
        "Exponential loss - np.exp(-1 * y * pred).sum() = 39.5090\n",
        "\n",
        "2b)\n",
        "\n",
        "Training accuracy - 83.54\n",
        "Testing accuracy - 68.82\n",
        "\n",
        "\n",
        "2c)\n",
        "\n"
      ],
      "metadata": {
        "id": "L4f-Sd1x29S1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Node(object):    \n",
        "    \n",
        "    def __init__(self, data, parent_attr_value):\n",
        "        self.data = data\n",
        "        self.children = []\n",
        "        self.attr = None\n",
        "        self.decision = None\n",
        "        self.parent_attr = None\n",
        "        self.parent_attr_value = parent_attr_value\n",
        "\n",
        "class DT(object):\n",
        "    \n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.root = None\n",
        "        self.training_error = None\n",
        "        \n",
        "    def classify(self):\n",
        "        node = Node(self.data, None)\n",
        "        self.root = node \n",
        "        return self.build_tree(node)\n",
        "        \n",
        "    def best_attribute(self, node):\n",
        "        data = node.data\n",
        "        M, N = data.shape;\n",
        "        \n",
        "        minimum_error = 1\n",
        "        best_attr = None\n",
        "        for attr in data.columns:\n",
        "            if attr == 'Y' or attr == 'W':\n",
        "                continue\n",
        "            \n",
        "            A_0pos_error = A_1pos_error = 0\n",
        "            attr_values = data[attr].unique()    \n",
        "            for attr_value in attr_values:\n",
        "                split = data[data[attr] == attr_value]\n",
        "                \n",
        "                W_p = (split[split['Y'] == 1]['W']).sum()\n",
        "                W_n = (split[split['Y'] == -1]['W']).sum()\n",
        "                \n",
        "                if attr_value == 0:                \n",
        "                    A_0pos_error = A_0pos_error + W_n\n",
        "                    A_1pos_error = A_1pos_error + W_p\n",
        "                elif attr_value == 1:\n",
        "                    A_0pos_error = A_0pos_error + W_p\n",
        "                    A_1pos_error = A_1pos_error + W_n\n",
        "            \n",
        "            error = min(A_0pos_error, A_1pos_error)\n",
        "            if error < minimum_error:\n",
        "                minimum_error = error\n",
        "                best_attr = attr\n",
        "                is_0p = A_0pos_error < A_1pos_error\n",
        "                \n",
        "        All_pos_error = data[data['Y'] == -1]['W'].sum()\n",
        "        All_neg_error = data[data['Y'] == 1]['W'].sum()\n",
        "        \n",
        "        All_pos = All_neg = False\n",
        "        if All_pos_error < minimum_error:\n",
        "            minimum_error = All_pos_error\n",
        "            All_pos = True\n",
        "            All_neg = False\n",
        "            \n",
        "        if All_neg_error < minimum_error:\n",
        "            minimum_error = All_neg_error\n",
        "            All_pos = False\n",
        "            All_neg = True\n",
        "                \n",
        "        return (best_attr, minimum_error, is_0p, All_pos, All_neg)\n",
        "        \n",
        "    def build_tree(self, node):        \n",
        "        data = node.data\n",
        "        (attr, error, is_0p, All_pos, All_neg) = self.best_attribute(node)\n",
        "        \n",
        "        if All_pos:\n",
        "            node.decision = 1\n",
        "            node.attr = 'POSITIVE'\n",
        "            return error\n",
        "        \n",
        "        if All_neg:\n",
        "            node.decision = -1\n",
        "            node.attr = 'NEGATIVE'\n",
        "            return error\n",
        "        \n",
        "        attr_values = data[attr].unique()    \n",
        "        node.attr = attr\n",
        "        \n",
        "        for attr_value in attr_values:\n",
        "            split = data[data[attr] == attr_value]\n",
        "            child = Node(split, attr_value)\n",
        "            child.parent_attr = attr\n",
        "            node.children.append(child)\n",
        "            \n",
        "            if attr_value == 0 and is_0p: \n",
        "                child.decision = 1\n",
        "            elif attr_value == 1 and is_0p:\n",
        "                child.decision = -1\n",
        "            elif attr_value == 0 and not is_0p:\n",
        "                child.decision = -1\n",
        "            elif attr_value == 1 and not is_0p:\n",
        "                child.decision = 1\n",
        "                \n",
        "        return error\n",
        "    \n",
        "    def predict_dataset(self, data):    \n",
        "        Y_pred = []\n",
        "        for i in range(0, len(data)):\n",
        "            prediction = self.predict(self.root, data.iloc[i])\n",
        "            Y_pred.append(prediction)\n",
        "        return Y_pred\n",
        "    \n",
        "    def predict(self, node, data_point):        \n",
        "        if node.decision is not None:\n",
        "            return node.decision    \n",
        "        attr = node.attr            \n",
        "        child = None\n",
        "        for child in node.children:        \n",
        "            if child.parent_attr_value == data_point[attr]:\n",
        "                break              \n",
        "        return self.predict(child, data_point)       \n",
        "         \n",
        "\n",
        "\n",
        "def update_weights(data, classifier, alpha, error):\n",
        "    normalizing_constant = (2.0)*(np.sqrt(error*(1-error)))\n",
        "    \n",
        "    Y = data['Y']\n",
        "    Y_pred = classifier.predict_dataset(data)\n",
        "    W = data['W']\n",
        "    \n",
        "    W_updated = ( (W)*(np.exp(-1.0 * Y*Y_pred*alpha))/normalizing_constant )\n",
        "    data['W'] = W_updated\n",
        "    \n",
        "    return data\n",
        "\n",
        "\n",
        "def adaboost(data_train, iterations):\n",
        "    # Initialize all weights to (1/M)\n",
        "    M, N = data_train.shape;\n",
        "    data = data_train\n",
        "    data['W'] = (np.ones(M)*(1.0/M)).reshape(M,1)\n",
        "    \n",
        "    classifiers = []\n",
        "    alphas = []\n",
        "    \n",
        "    for i in range(iterations):\n",
        "        # Select the classifier that has minimizes weighted misclassification error\n",
        "        classifier = DT(data)\n",
        "        error = classifier.classify()\n",
        "        \n",
        "        # Compute alpha\n",
        "        alpha = (0.5)*(np.log((1-error)/error))\n",
        "        \n",
        "        # Update weights\n",
        "        data = update_weights(data, classifier, alpha, error)\n",
        "        \n",
        "        alphas.append(alpha)\n",
        "        classifiers.append(classifier)\n",
        "        print('ATTR : ', i, classifier.root.attr)\n",
        "        \n",
        "    return (alphas, classifiers)\n",
        "\n",
        "def predict(data, alphas, classifiers):\n",
        "    \n",
        "    M, N = data.shape\n",
        "    Y = data['Y'].values.reshape(M,1)\n",
        "    Y_pred = np.zeros(M).reshape(M,1)\n",
        "    \n",
        "    N_c = len(classifiers)\n",
        "    \n",
        "    for i in range(N_c):\n",
        "        alpha = alphas[i]\n",
        "        Y_pred = Y_pred + alpha*(np.array(classifiers[i].predict_dataset(data)).reshape(M,1))\n",
        "        \n",
        "    \n",
        "    Y_pred[Y_pred < 0] = -1\n",
        "    Y_pred[Y_pred >= 0] = 1\n",
        "    #print(Y-Y_pred)\n",
        "    \n",
        "    misclassification = sum(abs(Y-Y_pred))/2\n",
        "    \n",
        "    return (1.0 - misclassification/M ) * 100\n",
        "    \n",
        "(alphas, classifiers) = adaboost(data, iterations=20)\n",
        "training_accuracy = predict(data, alphas, classifiers)\n",
        "print('ACCURACY ON TRAINING SET', training_accuracy)\n",
        "test_accuracy = predict(test_data, alphas, classifiers)\n",
        "print('ACCURACY ON TEST SET : ', test_accuracy)\n"
      ],
      "metadata": {
        "id": "rh9j90Osc0CF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E6EYqdwA2hTY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}